{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8530d245-7473-4431-b91b-736ddd9593e6",
   "metadata": {},
   "source": [
    "### Plan:\n",
    "1. **Get tokens** for positive and negative tweets (by `token` in this context we mean `word`).\n",
    "2. **Lemmatize** them (convert to base word forms). For that we will use a Part-of-Speech tagger.\n",
    "3. **Clean'em up** (remove mentions, URLs, stop words).\n",
    "4. **Prepare models** for the classifier, based on cleaned-up tokens.\n",
    "5. **Run the Naive Bayes classifier**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dae761c-bd27-4d32-be19-f07965ced307",
   "metadata": {},
   "source": [
    "First, download necessary prepared samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42819310-88ea-463b-b6a8-cd88a312c4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2ee03cc-4142-4362-85e6-508597ad4f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca88fbc7-e208-46cc-b748-eeb3a9191fe6",
   "metadata": {},
   "source": [
    "Get some sample positive/negative tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "727ac170-6412-41e6-b6c6-1038906ffbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import twitter_samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b8c01a-fffb-42dd-bfe5-c0cf24bb7c38",
   "metadata": {},
   "source": [
    "We can either get the actual string content of those tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25b8a199-3b39-4635-b658-375493490988",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae328604-e25e-4abf-a6b8-5d664f1438c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@groovinshawn they are rechargeable and it normally comes with a charger when u buy it :)'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_tweets[50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d184a10-a0be-477b-8e9a-36749a32730a",
   "metadata": {},
   "source": [
    "Or we can get a list of tokens using [tokenized method](https://www.nltk.org/howto/twitter.html) on `twitter_samples`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfd4b52e-d58e-4e5d-9275-0097185fbf8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@groovinshawn', 'they', 'are', 'rechargeable', 'and', 'it', 'normally', 'comes', 'with', 'a', 'charger', 'when', 'u', 'buy', 'it', ':)']\n"
     ]
    }
   ],
   "source": [
    "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "print(tweet_tokens[50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff367ad-5a64-4ce9-b81b-e7c9bc9103a1",
   "metadata": {},
   "source": [
    "Now let's setup a Part-of-Speech tagger.  Download a perceptron tagger that will be used by the PoS tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "313b4107-4729-43c4-bb72-31be6498e4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a170912-6686-4b4d-9e09-a6b56cb80e68",
   "metadata": {},
   "source": [
    "Import Part-of-Speech tagger that will be used for lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c64174c-0358-474c-a206-8f0038f7fb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea756f9e-c15b-402c-9f80-bad6de2c25f3",
   "metadata": {},
   "source": [
    "Check how it works. Note that it returns tuples, where second element is a Part-of-Speech identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a962113-b87c-40f9-896c-a2799447d4f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('@groovinshawn', 'NN'),\n",
       " ('they', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('rechargeable', 'JJ'),\n",
       " ('and', 'CC'),\n",
       " ('it', 'PRP'),\n",
       " ('normally', 'RB'),\n",
       " ('comes', 'VBZ'),\n",
       " ('with', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('charger', 'NN'),\n",
       " ('when', 'WRB'),\n",
       " ('u', 'JJ'),\n",
       " ('buy', 'VB'),\n",
       " ('it', 'PRP'),\n",
       " (':)', 'JJ')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(tweet_tokens[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f098854c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@groovinshawn',\n",
       " 'they',\n",
       " 'are',\n",
       " 'rechargeable',\n",
       " 'and',\n",
       " 'it',\n",
       " 'normally',\n",
       " 'comes',\n",
       " 'with',\n",
       " 'a',\n",
       " 'charger',\n",
       " 'when',\n",
       " 'u',\n",
       " 'buy',\n",
       " 'it',\n",
       " ':)']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_tokens[50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7f4c99-cb9a-40b1-a617-fe172051fa05",
   "metadata": {},
   "source": [
    "Let's write a function that will lemmatize twitter tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b70d698-6050-4bd9-b365-66ef10e6de66",
   "metadata": {},
   "source": [
    "For that, let's first fetch a WordNet resource. WordNet is a semantically-oriented dictionary of English - check chapter 2.5 of the NLTK book. In online version, this is part 5 [here](https://www.nltk.org/book/ch02.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "560e3e7b-a172-471c-8052-6dabbae85813",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04a3d4c-9080-4dbc-a077-2a6e234e03c8",
   "metadata": {},
   "source": [
    "Now fetch PoS tokens so that they can be passed to `WordNetLemmatizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a44c22c2-e6d3-47f4-a294-c3b78509d842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@groovinshawn', 'they', 'be', 'rechargeable', 'and', 'it', 'normally', 'come', 'with', 'a', 'charger', 'when', 'u', 'buy', 'it', ':)']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Припустимо, що tweet_tokens вже визначено\n",
    "tokens = tweet_tokens[50]\n",
    "\n",
    "# Створюємо лематизатор\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Функція для визначення частини мови для WordNet\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('NN'):  # Якщо іменник\n",
    "        return 'n'\n",
    "    elif tag.startswith('VB'):  # Якщо дієслово\n",
    "        return 'v'\n",
    "    else:\n",
    "        return 'a'  # Якщо інше, приймаємо як прикметник\n",
    "\n",
    "# Лематизуємо кожне слово\n",
    "lemmatized_sentence = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tag(tokens)]\n",
    "print(lemmatized_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba42f25-399f-40bf-a7c0-d0ff912a2f3f",
   "metadata": {},
   "source": [
    "Note that it converts words to their base forms ('are' -> 'be', 'comes' -> 'come').\n",
    "\n",
    "Now we can proceed to processing. \n",
    "During processing, we will perform cleanup:\n",
    "  - remove URLs and mentions using regexes\n",
    "  - after lemmatization, remove *stopwords*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "478174c9-8375-4722-88f7-8b551ed5ea59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e47fe6-b896-4583-b971-50764e7484f9",
   "metadata": {},
   "source": [
    "What are these stopwords? Let's see some."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac4a45d9-327b-4d06-beb1-259809529c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "i\n",
      "me\n",
      "my\n",
      "myself\n",
      "we\n",
      "our\n",
      "ours\n",
      "ourselves\n",
      "you\n",
      "you're\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "print(len(stop_words))\n",
    "\n",
    "for i in range(10):\n",
    "    print(stop_words[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bc3201-6336-4a39-9628-bf76bf612b32",
   "metadata": {},
   "source": [
    "Here comes the `process_tokens` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2862fbeb-2a75-4b09-997d-1767c07e0541",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "def process_tokens(tweet_tokens):\n",
    "    # Ініціалізуємо порожній список для очищених токенів\n",
    "    cleaned_tokens = []\n",
    "    \n",
    "    # Завантажуємо стоп-слова для англійської мови\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    # Створюємо лематизатор\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Проходимо по кожному токену та його тегу\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        # Регулярні вирази для виявлення URL та згадок користувачів\n",
    "        if (re.search(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', token) or \n",
    "            re.search(r'(@[A-Za-z0-9_]+)', token)):\n",
    "            continue  # Пропускаємо URL та згадки користувачів\n",
    "        \n",
    "        # Визначаємо частину мови для лематизації\n",
    "        if tag.startswith('NN'):  # Якщо іменник\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):  # Якщо дієслово\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'  # Якщо інше, приймаємо як прикметник\n",
    "   \n",
    "        # Виконуємо лематизацію\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        # Додаємо токен до очищених токенів, якщо він не є пунктуацією та не є стоп-словом\n",
    "        if token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    \n",
    "    # Повертаємо очищені токени\n",
    "    return cleaned_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728c7c49-46cf-4537-9fe4-750da9c38304",
   "metadata": {},
   "source": [
    "Let's test `process_tokens`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dafb0eb2-44c4-4d24-8bc8-2d97ed56e5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: ['@groovinshawn', 'they', 'are', 'rechargeable', 'and', 'it', 'normally', 'comes', 'with', 'a', 'charger', 'when', 'u', 'buy', 'it', ':)']\n",
      "After: ['rechargeable', 'normally', 'come', 'charger', 'u', 'buy', ':)']\n"
     ]
    }
   ],
   "source": [
    "print(\"Before:\", tweet_tokens[50])\n",
    "print(\"After:\", process_tokens(tweet_tokens[50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7880f61-72f3-4f61-b632-95df93e615dc",
   "metadata": {},
   "source": [
    "Run `process_tokens` on all positive/negative tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f0a4f99b-39e4-4764-83ab-8ad09dab51f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
    "\n",
    "positive_cleaned_tokens_list = [process_tokens(tokens) for tokens in positive_tweet_tokens]\n",
    "negative_cleaned_tokens_list = [process_tokens(tokens) for tokens in negative_tweet_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92716ef9-e8d1-43d3-bc94-621362601f60",
   "metadata": {},
   "source": [
    "Let's see how did the processing go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d90aa3b-6606-4492-901c-6f26519e926d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dang', 'that', 'is', 'some', 'rad', '@AbzuGame', '#fanart', '!', ':D', 'https://t.co/bI8k8tb9ht']\n",
      "['dang', 'rad', '#fanart', ':d']\n"
     ]
    }
   ],
   "source": [
    "print(positive_tweet_tokens[500])\n",
    "print(positive_cleaned_tokens_list[500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40a7964-ae39-42a5-b8da-a4a9ba9912da",
   "metadata": {},
   "source": [
    "Let's see what is most common there. Add a helper function `get_all_words`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "400ab380-4422-48bb-a513-27ca12d3ac87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_words(cleaned_tokens_list):\n",
    "    return [w for tokens in cleaned_tokens_list for w in tokens]\n",
    "\n",
    "all_pos_words = get_all_words(positive_cleaned_tokens_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515b74f0-34db-4295-b453-cb05dda8aefa",
   "metadata": {},
   "source": [
    "Perform frequency analysis using `FreqDist`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05fa8805-6013-496e-8673-51308d607e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(':)', 3691), (':-)', 701), (':d', 658), ('thanks', 388), ('follow', 357), ('love', 333), ('...', 290), ('good', 283), ('get', 263), ('thank', 253)]\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "freq_dist_pos = FreqDist(all_pos_words)\n",
    "print(freq_dist_pos.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055ad9c7-30dd-4f83-a307-7839fe03757e",
   "metadata": {},
   "source": [
    "Fine. Now we'll convert these to a data structure usable for NLTK's naive Bayes classifier ([docs here](https://www.nltk.org/_modules/nltk/classify/naivebayes.html)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "790f1f82-d35d-45d9-90b3-52682501eb7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#followfriday', 'top', 'engage', 'member', 'community', 'week', ':)']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tweet_tokens for tweet_tokens in positive_cleaned_tokens_list][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e171ca9-67a1-424a-92c8-88cb4e47d76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_dict(tokens):\n",
    "    return dict([token, True] for token in tokens)\n",
    "    \n",
    "def get_tweets_for_model(cleaned_tokens_list):   \n",
    "    return [get_token_dict(tweet_tokens) for tweet_tokens in cleaned_tokens_list]\n",
    "\n",
    "positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n",
    "negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d61171-054b-468f-bc92-e57494690f9d",
   "metadata": {},
   "source": [
    "Create two datasets for positive and negative tweets. Use 7000/3000 split for train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "18964303-5c20-4a88-bcd1-df5f7e703aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Розмір навчального датасету: 7000\n",
      "Розмір тестового датасету: 3000\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Створюємо навчальний датасет з позитивними та негативними твітами\n",
    "positive_dataset = [(tweet_dict, \"Positive\") for tweet_dict in positive_tokens_for_model]\n",
    "negative_dataset = [(tweet_dict, \"Negative\") for tweet_dict in negative_tokens_for_model]\n",
    "\n",
    "# Об'єднуємо обидва датасети\n",
    "dataset = positive_dataset + negative_dataset\n",
    "\n",
    "# Випадково перемішуємо датасет\n",
    "random.shuffle(dataset)\n",
    "\n",
    "# Розділяємо датасет на навчальні та тестові дані\n",
    "train_data = dataset[:7000]  # Перші 7000 твітів для навчання\n",
    "test_data = dataset[7000:]  # Решта для тестування\n",
    "\n",
    "# Перевірка результатів\n",
    "print(\"Розмір навчального датасету:\", len(train_data))\n",
    "print(\"Розмір тестового датасету:\", len(test_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb231b7-263f-45bb-abca-909bab161700",
   "metadata": {},
   "source": [
    "Finally we use the nltk's NaiveBayesClassifier on the training data we've just created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a928308c-c221-4047-a3ff-dd56b2686c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.996\n",
      "Most Informative Features\n",
      "                      :( = True           Negati : Positi =   2079.2 : 1.0\n",
      "                      :) = True           Positi : Negati =   1639.1 : 1.0\n",
      "                     sad = True           Negati : Positi =     52.7 : 1.0\n",
      "                     bam = True           Positi : Negati =     24.2 : 1.0\n",
      "                follower = True           Positi : Negati =     20.1 : 1.0\n",
      "                    glad = True           Positi : Negati =     19.5 : 1.0\n",
      "                     x15 = True           Negati : Positi =     17.8 : 1.0\n",
      "                  arrive = True           Positi : Negati =     17.3 : 1.0\n",
      "                 forward = True           Positi : Negati =     15.6 : 1.0\n",
      "              appreciate = True           Positi : Negati =     14.2 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
    "\n",
    "print(classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8218b77-b2b0-4a0b-81bc-9d90cb11b154",
   "metadata": {},
   "source": [
    "Note the Positive:Negative ratios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdb6996-3f21-4bbd-8a71-4e3c2f1eacdf",
   "metadata": {},
   "source": [
    "Let's check some test phrase. First, download punkt sentence tokenizer ([docs here](https://www.nltk.org/api/nltk.tokenize.punkt.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eab87fd4-9909-469b-8c0a-3c4fd3a36199",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Alex\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145ea904-26bb-4f64-9ec0-634ee5f1b7bd",
   "metadata": {},
   "source": [
    "Now we won't rely on `twitter_samples.tokenized`, but rather will use a generic tokenization routine - `word_tokenize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "01c24fbd-1dcd-4068-9831-c56c9482a636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Введений твіт\n",
    "custom_tweet = \"the service was #bad\"\n",
    "\n",
    "# Токенізуємо та очищуємо токени\n",
    "custom_tokens = process_tokens(word_tokenize(custom_tweet))\n",
    "\n",
    "# Класифікуємо твіт за допомогою класифікатора\n",
    "# І передаємо токени у функцію get_token_dict(), яка приймає словник токенів та повертає словник ознак\n",
    "# classifier.classify() використовує навчену модель для класифікації твіту\n",
    "print(classifier.classify(get_token_dict(custom_tokens)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878bbbbb-7db6-4d20-8d8b-0d8c48ff79e9",
   "metadata": {},
   "source": [
    "Let's package it as a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eea00802-a825-438b-b8ef-5c36519e6fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'bad', Сентимент: 'Negative'\n",
      "'service is bad', Сентимент: 'Negative'\n",
      "'service is really bad', Сентимент: 'Negative'\n",
      "'service is so terrible', Сентимент: 'Negative'\n",
      "'great service', Сентимент: 'Positive'\n",
      "'they stole my money', Сентимент: 'Negative'\n",
      "'#good', Сентимент: 'Positive'\n"
     ]
    }
   ],
   "source": [
    "def get_sentiment(text):\n",
    "    # Очищення та токенізація тексту\n",
    "    custom_tokens = process_tokens(word_tokenize(text))\n",
    "    \n",
    "    # Класифікація тексту за допомогою навченої моделі\n",
    "    sentiment = classifier.classify(get_token_dict(custom_tokens))\n",
    "    \n",
    "    # Повертаємо результат класифікації\n",
    "    return sentiment\n",
    "\n",
    "# Вхідні тексти для аналізу\n",
    "texts = [\"bad\", \"service is bad\", \"service is really bad\", \"service is so terrible\", \"great service\", \"they stole my money\", \"#good\"]\n",
    "\n",
    "# Виведення результатів класифікації для кожного вхідного тексту\n",
    "for t in texts:\n",
    "    sentiment = get_sentiment(t)\n",
    "    print(f\"'{t}', Сентимент: '{sentiment}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f574c3-ed8a-442c-9f65-bcb50551c817",
   "metadata": {},
   "source": [
    "Seems ok!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02aff76e-bc19-4c2b-836e-bdddd013faad",
   "metadata": {},
   "source": [
    "# Homework "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f60cd6dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('movie_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3d25d0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Імпорт модуля з відгуками на фільми\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "# Отримання списку файлових ідентифікаторів позитивних та негативних відгуків\n",
    "positive_reviews = movie_reviews.fileids('pos')  # Список файлових ідентифікаторів позитивних відгуків\n",
    "negative_reviews = movie_reviews.fileids('neg')  # Список файлових ідентифікаторів негативних відгуків"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "921f717e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Імпортуємо необхідний модуль\n",
    "import nltk\n",
    "# Імпортуємо корпус з відгуками на фільми\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "# Отримуємо список файлових ідентифікаторів позитивних та негативних відгуків\n",
    "positive = movie_reviews.fileids('pos')\n",
    "negative = movie_reviews.fileids('neg')\n",
    "\n",
    "# Розділення відгуків на позитивні та негативні\n",
    "positive_reviews = [nltk.corpus.movie_reviews.raw((positive[i])).replace(\"\\n\", \"\") for i in range(len(positive))]\n",
    "negative_reviews = [nltk.corpus.movie_reviews.raw((negative[i])).replace(\"\\n\", \"\") for i in range(len(negative))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d0ced973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Імпорт необхідних модулів\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Підготовка масивів з токенами для позитивних та негативних відгуків\n",
    "positive_tokens_for_model = get_tweets_for_model([process_tokens(word_tokenize(positive_reviews[i])) for i in range(len(positive_reviews))])\n",
    "negative_tokens_for_model = get_tweets_for_model([process_tokens(word_tokenize(negative_reviews[i])) for i in range(len(negative_reviews))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9b375673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Створення позитивної та негативної вибірки для моделі\n",
    "positive_dataset = [(token, \"Positive\") for token in positive_tokens_for_model]\n",
    "negative_dataset = [(token, \"Negative\") for token in negative_tokens_for_model]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1a92db",
   "metadata": {},
   "source": [
    "Now we won't rely on `twitter_samples.tokenized`, but rather will use a generic tokenization routine - `word_tokenize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "deb52343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Об'єднання позитивного та негативного датасетів\n",
    "test_dataset = positive_dataset + negative_dataset\n",
    "\n",
    "# Випадкове перемішування датасету\n",
    "random.shuffle(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f6876e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.596\n"
     ]
    }
   ],
   "source": [
    "# Визначення точності на вибірці\n",
    "print(\"Accuracy is:\", classify.accuracy(classifier, test_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48c969f",
   "metadata": {},
   "source": [
    "### Conclusion: the provided code snippets demonstrate a structured approach to sentiment analysis, including data preprocessing, model training, evaluation, and accuracy assessment. These are fundamental steps in building effective sentiment analysis systems for various applications, including social media monitoring, customer feedback analysis, and opinion mining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "22eeec32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функція для обробки токенів тестових твітів\n",
    "def process_tokens_test(tweet_tokens):\n",
    "    cleaned_tokens = []  # Ініціалізація списку для очищених токенів\n",
    "    stop_words = stopwords.words('english')  # Завантаження стоп-слів для англійської мови\n",
    "    lemmatizer = WordNetLemmatizer()  # Ініціалізація лематизатора\n",
    "\n",
    "    # Цикл обробки кожного токену та його тегу\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        # Умова для відкидання URL, згадок користувачів та хештегів\n",
    "        if (re.search(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', token) or \n",
    "            re.search(r'(@[A-Za-z0-9_]+)', token) or re.search(r'(#[A-Za-z0-9_]+)', token)):\n",
    "            continue  # Пропускаємо токен, якщо він є URL, згадкою користувача або хештегом\n",
    "\n",
    "        # Визначення частини мови для лематизації\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "   \n",
    "        token = lemmatizer.lemmatize(token, pos)  # Лематизація токену\n",
    "\n",
    "        # Додавання очищеного токену до списку, якщо він не є пунктуацією та не є стоп-словом\n",
    "        if token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens  # Повертаємо список очищених токенів\n",
    "\n",
    "# Отримання токенів позитивних та негативних твітів\n",
    "positive_tweet_tokens_test = twitter_samples.tokenized('positive_tweets.json')\n",
    "negative_tweet_tokens_test = twitter_samples.tokenized('negative_tweets.json')\n",
    "\n",
    "# Очищення та обробка токенів позитивних та негативних твітів\n",
    "positive_cleaned_tokens_list_test = [process_tokens_test(tokens) for tokens in positive_tweet_tokens_test]\n",
    "negative_cleaned_tokens_list_test = [process_tokens_test(tokens) for tokens in negative_tweet_tokens_test]\n",
    "\n",
    "# Підготовка токенів для моделі\n",
    "positive_tokens_for_model_test = get_tweets_for_model(positive_cleaned_tokens_list_test)\n",
    "negative_tokens_for_model_test = get_tweets_for_model(negative_cleaned_tokens_list_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c2027a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Створення позитивної та негативної вибірки для моделі\n",
    "positive_dataset_test = [(tweet_dict, \"Positive\") for tweet_dict in positive_tokens_for_model_test]\n",
    "negative_dataset_test = [(tweet_dict, \"Negative\") for tweet_dict in negative_tokens_for_model_test]\n",
    "\n",
    "# Об'єднання позитивного та негативного датасетів\n",
    "dataset_test = positive_dataset_test + negative_dataset_test\n",
    "\n",
    "# Випадкове перемішування датасету\n",
    "random.shuffle(dataset_test)\n",
    "\n",
    "# Розділення датасету на навчальні та тестові дані\n",
    "train_data_new = dataset_test[:7000]  # Перші 7000 твітів для навчання\n",
    "test_data_new = dataset_test[7000:]  # Решта для тестування"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5b76601d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Середня точність з використанням хештегів: 0.9982095555555555\n",
      "Середня точність без використання хештегів: 0.9989948888888889\n",
      "Середня точність без використання хештегів вище на: 0.0785 %\n"
     ]
    }
   ],
   "source": [
    "# Створення нового класифікатора на основі навчальних даних\n",
    "classifier_new = NaiveBayesClassifier.train(train_data_new)\n",
    "\n",
    "# Ініціалізація списків для збереження точності з та без використання хештегів\n",
    "acc_hashon = []\n",
    "acc_hashoff = []\n",
    "\n",
    "# Проведення порівняльного аналізу точності для 1500 ітерацій\n",
    "for i in range(1500):\n",
    "    # Випадкове перемішування датасету\n",
    "    random.shuffle(dataset)\n",
    "    \n",
    "    # Виділення тестових даних\n",
    "    test_data = dataset[7000:]\n",
    "    \n",
    "    # Оцінка точності з використанням класифікатора без урахування хештегів\n",
    "    acc_hashon.append(classify.accuracy(classifier, test_data))\n",
    "    \n",
    "    # Оцінка точності з використанням класифікатора з урахуванням хештегів\n",
    "    acc_hashoff.append(classify.accuracy(classifier_new, test_data))\n",
    "\n",
    "# Обчислення середньої точності з та без хештегів\n",
    "mean_accuracy_hashon = sum(acc_hashon)/len(acc_hashon)\n",
    "mean_accuracy_hashoff = sum(acc_hashoff)/len(acc_hashoff)\n",
    "\n",
    "# Виведення результатів\n",
    "print(\"Середня точність з використанням хештегів:\", mean_accuracy_hashon)\n",
    "print(\"Середня точність без використання хештегів:\", mean_accuracy_hashoff)\n",
    "\n",
    "# Порівняння середньої точності з та без хештегів та виведення результату\n",
    "if mean_accuracy_hashon > mean_accuracy_hashoff:\n",
    "    print(f\"Середня точність з використанням хештегів вище на: {round(100 * (mean_accuracy_hashon - mean_accuracy_hashoff), 4)} %\")\n",
    "else:\n",
    "    print(f\"Середня точність без використання хештегів вище на: {round(100 * (mean_accuracy_hashoff - mean_accuracy_hashon), 4)} %\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fd7e4d",
   "metadata": {},
   "source": [
    "### Conclusion: After conducting experiments with sentiment analysis models, it's evident that training a classifier on a dataset where hashtags are excluded leads to higher accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b1a58be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b759d3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 0.9963\n"
     ]
    }
   ],
   "source": [
    "# Створення списку ознак та міток з датасету\n",
    "features = [features for features, label in dataset]\n",
    "labels = [label for features, label in dataset]\n",
    "\n",
    "# Ініціалізація векторизатора\n",
    "vectorizer = DictVectorizer()\n",
    "\n",
    "# Ініціалізація класифікатора логістичної регресії\n",
    "log_reg_cls = LogisticRegression()\n",
    "\n",
    "# Створення конвеєру для векторизації та класифікації\n",
    "pipeline = make_pipeline(vectorizer, log_reg_cls)\n",
    "\n",
    "# Оцінка точності моделі за допомогою перехресної перевірки\n",
    "accuracy_scores = cross_val_score(pipeline, features, labels, cv=5, scoring='accuracy')\n",
    "\n",
    "# Виведення середньої точності\n",
    "print(\"Mean accuracy:\", accuracy_scores.mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "214de124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Середня точність моделі Naive Bayes вище, ніж середня точність моделі Logistic Regression.\n"
     ]
    }
   ],
   "source": [
    "# Порівняння середньої точності моделей Naive Bayes та Logistic Regression\n",
    "if accuracy_scores.mean() < sum(acc_hashon)/len(acc_hashon):\n",
    "    print(f\"Середня точність моделі Naive Bayes вище, ніж середня точність моделі Logistic Regression.\")\n",
    "else:\n",
    "    print(f\"Середня точність моделі Logistic Regression вище, ніж середня точність моделі Naive Bayes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2723f749",
   "metadata": {},
   "source": [
    "### Conclusion: Based on the experiments conducted, it appears that the NaiveBayesClassifier outperforms the LogisticRegression model in this particular scenario.The Naive Bayes classifier achieved higher mean accuracy compared to Logistic Regression when evaluated on the dataset. This suggests that for sentiment analysis tasks with the provided dataset and features, the Naive Bayes approach may be more suitable and effective."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
